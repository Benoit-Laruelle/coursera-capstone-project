1) DOWNLOAD AND CLEAN DATASET:

#!pip install numpy==1.21.4
#!pip install pandas==1.3.4
#!pip install keras==2.1.6

import pandas as pd
import numpy as np


2) DOWNLOAD THE DATA AND READ IT INTO A PANDAS DATAFRAME:

concrete_data = pd.read_csv('https://cocl.us/concreta_data')
concrete_data.head()

The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:
a. Cement
b. Blast Furnace Slag
c. Fly Ash
d. Water
e. Superplasticizer
f. Coarse Aggregate
g. Fine Aggregate


3) TO CHECK HOW MANY DATAPOINTS WE HAVE:

concrete_data.shape


4) TO CHECK THE DATASET FOR ANY MISSING VALUES:

concrete_data.describe()

concrete_data.isnull().sum()


5) SPLIT DATA INTO PREDICTORS AND TARGET:

concrete_data_columns = concrete_data.columns

predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength
target = concrete_data['Strength'] # Strength column


6) TO DO A SANITY CHECK OF THE PREDICTORS AND THE TARGET DATAFRAMES:

predictors.head()

target.head()


7) NORMALIZE THE DATA BY SUBTRACTING THE MEAN AND DIVIDING BY THE STANDARD DEVIATION:

predictors_norm = (predictors - predictors.mean()) / predictors.std()
predictors_norm.head()


8) SAVE THE NUMBER OF PREDICTORS TO n_cols WE WILL NEED THIS NUMBER WHEN BUILDING OUR NETWORK:

n_cols = predictors_norm.shape[1] # number of predictors


9) IMPORT KERAS LIBRARY: (TENSOR FLOW BACKEND IS USED TO INSTALL THE KERAS LIBRARY)

import keras
from keras.models import Sequential
fromkeras.layers import Dense


10) BUILD A NEURAL NETWORK: (REGRESSION MODEL THAT HAS 2 HIDDEN LAYERS, EACH OF 50 HIDDEN UNITS)

def regression_model():

model = sequential()

n_cols = concrete_data.shape[1]

model.add(Dense(50, activation=´relu´, input_shape=(n_cols,)))
model.add(Dense(50, activation=´relu´))
model.add(Dense(1))

model.compile(optimizer=´adam´, loss=´mean_square_error´)


11) TRAIN AND TEST THE NETWORK (USING THE FIT METHOD. LEAVE OUT 30% OF THE DATA FOR VALIDATION AND TRAIN THE MODEL FOR100 EPOCHS)

model = regression_model()

model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2)


12) PREDICTION / EVALUATION:

predictions = model.predict(test_data)
